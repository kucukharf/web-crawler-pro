{
    "author": "Burak Arslan",
    "dependencies": {
        "archiver": "^1.2.0",
        "bluebird": "^3.0.1",
        "cheerio": "0.22.0",
        "css-url-parser": "^1.0.0",
        "debug": "^2.4.5",
        "fs-extra": "^3.0.0",
        "he": "^1.1.0",
        "lodash": "^4.11.1",
        "normalize-url": "^1.5.3",
        "parse-domain": "^1.1.0",
        "prompt": "^1.0.0",
        "request": "^2.42.0",
        "rimraf": "^2.6.1",
        "srcset": "^1.0.0",
        "string-template": "^1.0.0",
        "time-stamp": "^1.0.1",
        "valid-url": "^1.0.9"
    },
    "description": "Website which crawl websites. Uses nodejs",
    "devDependencies": {
        "codeclimate-test-reporter": "^0.4.0",
        "coveralls": "^2.11.8",
        "eslint": "^3.9.1",
        "istanbul": "^0.4.0",
        "mocha": "^3.0.2",
        "nock": "^9.0.2",
        "proxyquire": "^1.7.3",
        "should": "^11.1.0",
        "sinon": "^2.1.0"
    },
    "files": [
        "index.js",
        "lib"
    ],
    "homepage": "https://github.com/kucukharf/web-crawler",
    "license": "MIT",
    "main": "index.js",
    "name": "web-crawler",
    "readmeFilename": "README.md",
    "repository": {
        "type": "git",
        "url": "git://github.com/kucukharf/web-crawler.git"
    },
    "scripts": {
        "clean": "node clean.js",
        "crawl": "export DEBUG=website-scraper*; node --max_old_space_size=6144 index.js",
        "eslint": "eslint lib/** index.js",
        "load": "node bar.js",
        "start": "node index.js",
        "test": "istanbul cover node_modules/mocha/bin/_mocha --dir ./coverage --report lcov -- -R spec --recursive --timeout 7000 ./test/unit/ ./test/functional && npm run eslint",
        "test-e2e": "node_modules/mocha/bin/_mocha --timeout 300000 ./test/e2e/*-test.js"
    },
    "version": "0.0.1"
}